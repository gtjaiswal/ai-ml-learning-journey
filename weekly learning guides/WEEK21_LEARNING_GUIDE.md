# WEEK 21 LEARNING GUIDE: Human-in-Loop + Phase 2 Completion

**Timeline:** April 6-12, 2026  
**Total Time:** ~11-12 hours  
**Focus:** Approval workflows, four-eyes principle, feedback loops, Phase 2 integration

---

## ðŸ“‹ WEEK OVERVIEW

**What You'll Build:**
- Approval queue system
- Four-eyes review workflow
- Feedback integration loop
- Active learning pipeline
- Complete Phase 2 integration
- Professional documentation suite
- Portfolio materials
- Phase 2 completion deliverables

**What You'll Learn:**
- Human-in-loop (HITL) design patterns
- Banking four-eyes principle implementation
- Approval workflow state machines
- Feedback collection and integration
- Active learning loop architecture
- System integration strategies
- Professional documentation standards
- Portfolio presentation techniques

**Fintech Application - CRITICAL:**

**Banking Regulation:**
```
"High-value or high-risk decisions require approval from 
two independent parties before execution"

Example:
Agent flags transaction (85% confidence) â†’ REJECT
â†“
Analyst 1 reviews â†’ Approves rejection
â†“
Analyst 2 reviews (can't see Analyst 1's decision) â†’ Approves rejection
â†“
Transaction rejected (four-eyes compliance)

NO automated execution of high-risk decisions
```

**Why This Matters:**
- **Regulatory Compliance:** SOX, Basel III, Dodd-Frank requirements
- **Risk Management:** Two independent checks reduce errors
- **Audit Trail:** Complete decision history for regulators
- **Liability Protection:** Shared responsibility for decisions
- **Customer Protection:** Appeals process with human oversight

---

## DAY 1 (MONDAY): Human-in-Loop Patterns

**Time:** 1.5 hours

### SESSION 1: HITL Fundamentals (45 min)

**Learning Resources:**

**Reading:**
- "Human-in-the-Loop Machine Learning" - Manning Publications (Chapter 1)
- URL: https://www.manning.com/books/human-in-the-loop-machine-learning
- Duration: 30 min (sample chapter)
- Focus: HITL patterns, when to use human review

**Video:**
- "Human-in-the-Loop AI Systems" - Weights & Biases
- URL: https://www.youtube.com/watch?v=Yqj7Kyjznh4
- Duration: 15 min
- Focus: HITL design principles

**What You Need to Understand:**

**When to Use HITL:**
1. **High-stakes decisions:** Financial transactions, medical diagnosis
2. **Low confidence predictions:** Model uncertainty above threshold
3. **Edge cases:** Unusual scenarios model hasn't seen
4. **Regulatory requirements:** Banking compliance, legal decisions
5. **Active learning:** Collecting training data from production

**HITL Patterns:**

**Pattern 1: Confidence-Based Routing**
- Low confidence (< 60%) â†’ Manual review
- Medium confidence (60-80%) â†’ Expedited review
- High confidence (> 80%) â†’ Auto-approve
- Document: Threshold selection methodology

**Pattern 2: Sampling for Auditing**
- Random sample of auto-approved for human check
- Ensures model quality over time
- Detects drift or degradation
- Document: Sampling strategy

**Pattern 3: Expert Review for Edge Cases**
- Specific scenarios always need expert review
- Examples: International transactions, New merchant categories, Amounts > $10K
- Document: Edge case definitions

**Pattern 4: Active Learning Loop**
- Human corrections become training data
- Continuous model improvement
- Workflow: Prediction â†’ Review â†’ Correction â†’ Training â†’ Deployment
- Document: Learning loop architecture

**Pattern 5: Escalation Workflow**
- Junior analyst â†’ Senior analyst â†’ Manager
- Complexity-based routing
- Document: Escalation criteria

**Pattern 6: Collaborative Review**
- Multiple reviewers see same case
- Agreement required for decision
- Banking four-eyes principle
- Document: Collaboration protocol

### SESSION 2: Four-Eyes Principle (45 min)

**Requirements:**

Design four-eyes approval system:

**Principle Overview:**
- Two independent approvals required
- Reviewers cannot see each other's decisions until both complete
- Ensures independence and reduces bias
- Required for high-risk/high-value decisions in banking

**Implementation Requirements:**

**Requirement 1: Independence**
- Requirements: Analyst 2 cannot see Analyst 1's decision
- What to figure out: How to hide Analyst 1's choice in UI
- Why critical: Prevents bias, ensures true independence
- Document: Independence enforcement mechanism

**Requirement 2: Agreement Handling**
- Requirements: Define what happens when both approve/reject
- Both approve â†’ Execute action
- Both reject â†’ Reject action
- What to figure out: Who decides? Escalate? Third reviewer?
- Document: Agreement resolution logic

**Requirement 3: Disagreement Handling**
- Requirements: Define what happens when reviewers disagree
- Option A: Escalate to manager
- Option B: Require third reviewer (tiebreaker)
- Option C: Default to conservative decision (reject)
- What to figure out: Which option for which scenarios
- Document: Disagreement resolution protocol

**Requirement 4: Reviewer Assignment**
- Requirements: How to assign reviewers fairly
- Constraints: Different departments (segregation of duties), Different seniority levels, No conflict of interest, Balanced workload
- What to figure out: Assignment algorithm
- Document: Assignment logic

**Requirement 5: Audit Trail**
- Requirements: Complete decision history
- Log: Who reviewed, When reviewed, What decision, Reasoning provided, How long it took
- Compliance: Prove two independent reviews occurred
- What to figure out: Audit log retention period (7+ years)
- Document: Audit requirements

**Success Criteria:**
- Four-eyes principle understood
- Independence mechanisms designed
- Agreement/disagreement handling defined
- Audit trail comprehensive
- Regulatory compliance verified

---

## DAY 2 (TUESDAY): Approval Queue System

**Time:** 1.5 hours

### SESSION 1: Queue Architecture (45 min)

**Requirements:**

Design approval queue infrastructure:

**Component 1: Database Schema**
- Requirements: Store pending approvals with all metadata
- Table 1 - approval_queue: id, transaction_id, agent_prediction, confidence, risk_score, status (PENDING/APPROVED/REJECTED/ESCALATED), created_at, assigned_to, priority (HIGH/MEDIUM/LOW), deadline
- Table 2 - approvals: id, queue_item_id, reviewer_id, decision, reasoning, reviewed_at, time_spent
- Table 3 - audit_log: id, queue_item_id, action, user_id, timestamp, details
- What to figure out: Indexing strategy for performance
- Document: Schema design rationale

**Component 2: Queue Manager**
- Requirements: Core queue operations
- Method 1 - enqueue(transaction, prediction): Add to queue
- Method 2 - assign(queue_item_id, analyst_id): Assign to reviewer
- Method 3 - approve(queue_item_id, analyst_id, reasoning): Record approval
- Method 4 - reject(queue_item_id, analyst_id, reasoning): Record rejection
- Method 5 - escalate(queue_item_id, reason): Move to manager queue
- Method 6 - get_pending(analyst_id): Get analyst's queue
- What to figure out: Concurrency handling (two analysts can't claim same item)
- Document: API design

**Component 3: Assignment Logic**
- Requirements: Intelligent work distribution
- Strategy 1 - Round-robin: Simple rotation
- Strategy 2 - Least busy: Assign to analyst with fewest pending
- Strategy 3 - Skill-based: Match complexity to analyst expertise
- Strategy 4 - Priority: High-priority items first
- What to figure out: Which strategy when
- Document: Assignment algorithm

**Component 4: SLA Monitoring**
- Requirements: Track time-to-resolution
- SLA 1 - High priority: < 1 hour
- SLA 2 - Medium priority: < 4 hours
- SLA 3 - Low priority: < 24 hours
- Alert when approaching deadline
- What to figure out: Escalation if SLA breached
- Document: SLA management

**Component 5: Audit Logging**
- Requirements: Log every action for compliance
- Events: Enqueued, Assigned, Reviewed, Approved, Rejected, Escalated, Deadline missed
- Fields: Timestamp, User, Action, Before/after state, Reasoning
- What to figure out: Log retention and archival
- Document: Audit trail requirements

**Success Criteria:**
- Queue architecture designed
- Database schema complete
- Assignment logic defined
- SLA monitoring planned
- Audit logging comprehensive

### SESSION 2: Four-Eyes Implementation (45 min)

**Requirements:**

Implement four-eyes workflow:

**Step 1: Enqueue for Review**
- Requirements: Agent prediction triggers approval workflow
- Trigger: confidence < 80% OR risk_score > 70 OR amount > $5000
- Create: Two queue items (one per required reviewer)
- Status: PENDING_FIRST_REVIEW and PENDING_SECOND_REVIEW
- What to figure out: How to link the two reviews
- Document: Triggering logic

**Step 2: First Review**
- Requirements: Analyst 1 reviews and decides
- UI shows: Transaction details, Agent prediction, Risk analysis, NO Analyst 2's decision (hidden)
- Analyst provides: Decision (APPROVE/REJECT), Reasoning (required)
- Update: queue_item_1 status to COMPLETED_FIRST_REVIEW
- What to figure out: How to prevent Analyst 2 from seeing decision
- Document: Review UI requirements

**Step 3: Second Review (Independent)**
- Requirements: Analyst 2 reviews independently
- UI shows: Same transaction details, Agent prediction, NO Analyst 1's decision
- Analyst provides: Decision (APPROVE/REJECT), Reasoning (required)
- Update: queue_item_2 status to COMPLETED_SECOND_REVIEW
- What to figure out: Ensuring true independence
- Document: Independence verification

**Step 4: Reconciliation**
- Requirements: Compare decisions and execute
- Both APPROVE â†’ Execute agent recommendation
- Both REJECT â†’ Reject agent recommendation
- Disagreement â†’ Escalate to manager queue
- Log: Final decision, Both reasonings, Execution result
- What to figure out: Manager override permissions
- Document: Reconciliation logic

**Step 5: Audit Trail**
- Requirements: Complete decision history
- Record: Both analysts' IDs, Both decisions, Both reasonings, Timestamps (submitted, reviewed), Final outcome, Execution details
- Compliance: Digital signatures (analyst IDs + timestamps)
- What to figure out: Report generation for auditors
- Document: Audit report format

**Success Criteria:**
- Four-eyes workflow implemented
- Independence guaranteed
- Reconciliation logic working
- Audit trail complete
- Compliance verified

---

## DAY 3 (WEDNESDAY): Feedback Integration

**Time:** 1.5 hours

### SESSION 1: Feedback Collection (45 min)

**Requirements:**

Build feedback capture system:

**Feedback Type 1: Correct/Incorrect Flag**
- Requirements: Binary feedback on agent decision
- UI: Simple âœ“/âœ— buttons
- Data: queue_item_id, correct (boolean), analyst_id
- Use case: Track accuracy in production
- What to figure out: How to handle subjective cases
- Document: Feedback schema

**Feedback Type 2: Risk Score Adjustment**
- Requirements: Analyst provides corrected risk score
- UI: Slider 0-100 with agent's score shown
- Data: queue_item_id, agent_score, analyst_score, difference
- Use case: Calibrate risk model
- What to figure out: When adjustments are significant (> 20 points?)
- Document: Adjustment thresholds

**Feedback Type 3: New Red Flag**
- Requirements: Analyst identifies signal agent missed
- UI: Multi-select from list + free text for new flags
- Data: queue_item_id, missed_red_flags (list), new_flags (list)
- Use case: Improve agent's red flag detection
- What to figure out: Categorization taxonomy
- Document: Red flag catalog

**Feedback Type 4: Missing Context**
- Requirements: Analyst provides context agent couldn't access
- UI: Text area for notes
- Examples: "Customer traveling for work", "Merchant name changed recently", "Seasonal purchase pattern"
- Use case: Identify data gaps
- What to figure out: How to make context actionable
- Document: Context categories

**Feedback Type 5: Explanation Quality**
- Requirements: Rate agent's reasoning clarity
- UI: 1-5 stars rating
- Data: queue_item_id, reasoning_rating, comments
- Use case: Improve prompt engineering
- What to figure out: What makes "good" reasoning
- Document: Rating criteria

**Feedback Interface Requirements:**
- Fast input (keyboard shortcuts)
- Structured fields (dropdowns, checkboxes)
- Optional free text for nuance
- Auto-save (don't lose work)
- Bulk actions (same feedback for multiple cases)

**Success Criteria:**
- 5 feedback types designed
- UI requirements documented
- Data schema defined
- Collection workflow smooth

### SESSION 2: Active Learning Loop (45 min)

**Requirements:**

Build continuous improvement pipeline:

**Step 1: Feedback Aggregation**
- Requirements: Collect and organize analyst corrections
- Storage: feedback database table
- Deduplication: Handle conflicting feedback (multiple analysts, same case)
- What to figure out: How to resolve conflicts (majority vote? expert priority?)
- Document: Aggregation logic

**Step 2: Pattern Detection**
- Requirements: Identify systematic errors
- Analysis: Which transaction types most often corrected?, Which red flags most often missed?, Which risk scores most often adjusted?
- Statistical tests: Significantly higher error rate for specific patterns?
- What to figure out: How to define "systematic" (> 20% error rate?)
- Document: Pattern analysis methodology

**Step 3: Training Data Generation**
- Requirements: Convert feedback into training examples
- Format: {"transaction": {...}, "correct_decision": "REJECT", "correct_risk_score": 85, "correct_red_flags": [...]}
- Quality filter: Only use high-confidence corrections (analyst confidence > 80%)
- What to figure out: How many examples needed before retraining (100? 500? 1000?)
- Document: Data generation pipeline

**Step 4: Model Improvement**
- Requirements: Retrain agent on feedback data
- Schedule: Weekly? Monthly? After N corrections?
- Validation: Test on holdout set before deploying
- Rollback: Keep previous version if performance degrades
- What to figure out: Continuous vs periodic retraining
- Document: Retraining workflow

**Step 5: Performance Tracking**
- Requirements: Measure improvement over time
- Metrics: Error rate before/after retraining, Specific error types reduced?, Analyst agreement rate increased?
- Visualization: Trend charts showing improvement
- What to figure out: How to communicate improvements to stakeholders
- Document: Improvement reporting

**Active Learning Workflow:**
```
Agent Predicts
    â†“
Human Reviews & Corrects
    â†“
Correction Stored in Database
    â†“
Weekly/Monthly Analysis
    â†“
Pattern Detection (systematic errors)
    â†“
Training Data Generation
    â†“
Model Retraining
    â†“
Validation on Holdout Set
    â†“
Deploy if Better (else rollback)
    â†“
Repeat
```

**Success Criteria:**
- Feedback aggregation working
- Patterns detected automatically
- Training data generated correctly
- Retraining workflow defined
- Performance tracking implemented

---

## DAY 4 (THURSDAY): Phase 2 Integration

**Time:** 1.5 hours

### SESSION 1: Component Integration (45 min)

**Requirements:**

Integrate all Phase 2 components:

**Integration 1: Multi-Agent System (Week 17)**
- Requirements: Add approval queue after decision agent
- Flow: Supervisor â†’ Workers â†’ Decision Agent â†’ Confidence Check â†’ If < 80%: Approval Queue, Else: Execute
- What to figure out: Where in orchestration to inject queue
- Document: Integration point

**Integration 2: Vision Agents (Week 18)**
- Requirements: Table extraction feeds fraud detection
- Flow: PDF â†’ Vision Agent (extract table) â†’ Data Agent (analyze) â†’ Risk Agent (score) â†’ Decision Agent â†’ Queue if needed
- What to figure out: How to pass extracted data between agents
- Document: Data flow

**Integration 3: Structured Outputs (Week 19)**
- Requirements: All queue entries use Pydantic models
- Models: QueueItem (typed), ApprovalDecision (typed), FeedbackEntry (typed)
- Benefit: Type safety across queue operations
- What to figure out: Schema versioning for queue database
- Document: Type contracts

**Integration 4: Evaluation Framework (Week 20)**
- Requirements: Track approval queue performance
- Metrics: Approval rate (% approved by analysts), Analyst agreement rate (% where both agree), SLA compliance (% resolved within SLA), Feedback quality (ratings distribution)
- What to figure out: How to evaluate human+AI performance
- Document: Combined evaluation strategy

**Integration 5: Human-in-Loop (Week 21)**
- Requirements: Complete approval workflow
- Components: Queue manager, Four-eyes workflow, Feedback collection, Active learning
- End-to-end: Agent â†’ Queue â†’ Review â†’ Approve â†’ Execute â†’ Feedback â†’ Improve
- What to figure out: Testing strategy for full workflow
- Document: Integration architecture

**Success Criteria:**
- All 5 integrations designed
- Data flow mapped
- API contracts defined
- Testing strategy documented

### SESSION 2: End-to-End Testing (45 min)

**Requirements:**

Test complete integrated system:

**Test Scenario 1: High-Confidence Auto-Approval**
- Input: Clear legitimate transaction, high confidence (95%)
- Expected: Agent approves automatically, No queue entry, Executes immediately
- Verify: No human review triggered, Audit log complete, Fast execution (< 2s)
- What to figure out: Edge case where high confidence but should review
- Document: Auto-approval criteria

**Test Scenario 2: Low-Confidence Human Review**
- Input: Borderline transaction, low confidence (65%)
- Expected: Queued for review, Assigned to analyst, Analyst reviews, Decision executed
- Verify: Queue entry created, Assignment correct, Review UI works, Decision logged
- What to figure out: Queue latency impact
- Document: Review workflow

**Test Scenario 3: Four-Eyes Approval**
- Input: High-risk transaction (amount > $10K)
- Expected: Two queue entries, Analyst 1 reviews independently, Analyst 2 reviews independently, Reconciliation executes
- Verify: Both reviews independent, Agreement handling correct, Audit trail complete
- What to figure out: Disagreement resolution in practice
- Document: Four-eyes test cases

**Test Scenario 4: Disagreement Escalation**
- Input: Transaction where analysts disagree
- Expected: Analyst 1 approves, Analyst 2 rejects, System escalates to manager, Manager resolves
- Verify: Escalation triggered, Manager queue entry, Resolution executed
- What to figure out: Manager override permissions
- Document: Escalation workflow

**Test Scenario 5: Feedback Integration**
- Input: Analyst corrects agent decision
- Expected: Feedback stored, Pattern detected if systematic, Training data generated, Model improves
- Verify: Feedback captured, Analysis runs, Training triggered (if threshold met)
- What to figure out: Improvement measurement
- Document: Feedback loop verification

**Test Scenario 6: Vision + Fraud Detection**
- Input: PDF with financial table
- Expected: Vision extracts table, Fraud agent analyzes, Queue if low confidence
- Verify: Table extracted accurately, Data passed correctly, Fraud analysis uses table data
- What to figure out: Error handling if table extraction fails
- Document: Multimodal workflow

**Performance Tests:**
- Load: 100 concurrent transactions
- Verify: Queue handles load, No race conditions, SLA maintained, Audit logs complete
- What to figure out: Bottlenecks and scaling limits
- Document: Performance characteristics

**Success Criteria:**
- All 6 scenarios tested
- End-to-end flow working
- Performance acceptable
- Errors handled gracefully
- Audit trail complete

---

## DAY 5 (FRIDAY): Documentation Day

**Time:** 1 hour

### SESSION: Professional Documentation (60 min)

**Requirements:**

Create 5 professional documents:

**Document 1: PHASE2_ARCHITECTURE.md**
- Requirements: Complete system architecture documentation
- Sections: System overview, Component diagram, Data flow, Integration points, Technology stack
- Diagrams: High-level architecture, Component relationships, Data flow
- What to figure out: Best visualization approach
- Audience: Technical stakeholders, future maintainers
- Document: Complete architecture reference

**Document 2: PHASE2_COMPLETION_SUMMARY.md**
- Requirements: Phase 2 summary and outcomes
- Sections: What was built (7 weeks of systems), Key learnings (technical + fintech), Challenges overcome (integration complexity, four-eyes implementation), Portfolio impact (interview-ready systems), Metrics (hours invested, systems built, skills gained)
- What to figure out: How to present achievements compellingly
- Audience: Recruiters, hiring managers
- Document: Executive summary

**Document 3: DEPLOYMENT_GUIDE.md**
- Requirements: Complete deployment instructions
- Sections: Prerequisites (dependencies, accounts), Installation (step-by-step), Configuration (environment variables, secrets), Deployment (dev, staging, production), Verification (health checks, smoke tests)
- What to figure out: Docker vs native deployment
- Audience: DevOps, platform engineers
- Document: Runbook for deployment

**Document 4: API_DOCUMENTATION.md**
- Requirements: Complete API reference
- For each endpoint: HTTP method, Path, Parameters (path, query, body), Request body (schema, example), Response body (schema, example), Error codes (all possible errors), Examples (curl, Python)
- Tools: OpenAPI spec generation
- What to figure out: Authentication documentation
- Audience: Integration teams, API consumers
- Document: API contract

**Document 5: MONITORING_RUNBOOK.md**
- Requirements: Operations and troubleshooting guide
- Sections: Metrics to watch (SLA, accuracy, latency, errors), Alert thresholds (when to act), Troubleshooting (common issues, solutions), On-call procedures (escalation, contacts)
- What to figure out: Common failure modes
- Audience: On-call engineers, support team
- Document: Operations manual

**Documentation Standards:**
- Clear, concise writing
- Examples for everything
- Diagrams where helpful
- Code snippets (requirements, not full code)
- Troubleshooting sections
- Version history

**Success Criteria:**
- All 5 documents created
- Professional quality
- Comprehensive coverage
- Easy to follow
- Stakeholder-appropriate

---

## DAY 6 (SATURDAY): Portfolio Materials

**Time:** 2.5 hours

### SESSION 1: Architecture Diagrams (90 min)

**Requirements:**

Create 6 professional diagrams:

**Diagram 1: Phase 2 System Architecture**
- Requirements: Complete system overview
- Components: Multi-agent system, Vision agents, Structured outputs, Evaluation framework, Approval queue, Feedback loop
- Connections: Data flow, API calls, Database interactions
- Tools: draw.io, Lucidchart, Mermaid
- What to figure out: Right level of detail (high-level vs detailed)
- Export: PNG + SVG
- Document: Architecture overview

**Diagram 2: Multi-Agent Workflow**
- Requirements: Supervisor-worker pattern detail
- Components: Supervisor, Data Agent, Pattern Agent, Risk Agent, Decision Agent
- Flow: Query â†’ Route â†’ Parallel work â†’ Aggregate â†’ Decide
- What to figure out: How to show parallelism visually
- Document: Agent orchestration

**Diagram 3: Human-in-Loop Workflow**
- Requirements: Approval queue flow
- States: PENDING â†’ ASSIGNED â†’ IN_REVIEW â†’ APPROVED/REJECTED â†’ EXECUTED
- Actors: Agent, Analyst 1, Analyst 2, Manager
- Decision points: Confidence check, Four-eyes required?, Agreement?
- What to figure out: State machine visualization
- Document: Workflow states

**Diagram 4: Feedback Loop**
- Requirements: Active learning cycle
- Flow: Predict â†’ Review â†’ Correct â†’ Store â†’ Analyze â†’ Generate Data â†’ Retrain â†’ Validate â†’ Deploy
- Metrics: Before/after accuracy
- What to figure out: How to show continuous improvement
- Document: Learning loop

**Diagram 5: Data Flow**
- Requirements: Transaction journey through system
- Path: Transaction â†’ Multi-agent â†’ Vision (if needed) â†’ Structured output â†’ Queue (if needed) â†’ Execution
- Data transformations: Raw â†’ Analyzed â†’ Scored â†’ Formatted â†’ Approved â†’ Executed
- What to figure out: Where data persisted
- Document: End-to-end flow

**Diagram 6: Deployment Architecture**
- Requirements: Production infrastructure
- Components: Load balancer, API servers, Queue workers, Database, Redis cache, Monitoring
- Connections: Traffic flow, Redundancy, Auto-scaling
- What to figure out: AWS services mapping
- Document: Production setup

**Success Criteria:**
- All 6 diagrams created
- Professional appearance
- Clear and readable
- Technically accurate
- Portfolio-ready

### SESSION 2: Demo & Presentation (60 min)

**Requirements:**

Prepare portfolio demonstration:

**Deliverable 1: Demo Script (10 minutes)**
- Requirements: Complete system walkthrough
- Section 1 (2 min): Transaction analysis - Show high-confidence auto-approval
- Section 2 (3 min): Low-confidence approval queue - Dashboard, Review interface, Four-eyes workflow
- Section 3 (2 min): Feedback integration - Analyst correction, System learns, Improvement shown
- Section 4 (2 min): Evaluation results - Accuracy metrics, Calibration curve, Performance improvements
- Section 5 (1 min): Architecture - Component diagram walkthrough
- What to figure out: What to show vs what to skip
- Practice: Rehearse multiple times
- Document: Demo script with screenshots

**Deliverable 2: Presentation Slides (15 slides max)**
- Requirements: Professional slide deck
- Slide 1: Title (Phase 2: AI Agents for Fintech)
- Slide 2: Problem (Manual fraud review inefficient)
- Slide 3: Solution (AI + human collaboration)
- Slide 4: Architecture (high-level diagram)
- Slide 5: Multi-Agent System (Week 17)
- Slide 6: Vision Integration (Week 18)
- Slide 7: Structured Outputs (Week 19)
- Slide 8: Evaluation Framework (Week 20)
- Slide 9: Human-in-Loop (Week 21)
- Slide 10: Feedback Loop (active learning)
- Slide 11: Metrics (94% accuracy, 50-case test suite, 0.04 ECE)
- Slide 12: Fintech Impact (regulatory compliance, four-eyes principle)
- Slide 13: Tech Stack (Python, FastAPI, LangChain, Pydantic, OpenAI, LangSmith)
- Slide 14: Portfolio Impact (production-grade systems)
- Slide 15: Next Steps (Week 22: Fine-tuning, Month 7: Certification)
- What to figure out: Visual style, Branding
- Tools: PowerPoint, Google Slides, Canva
- Document: Presentation deck

**Deliverable 3: Interview Stories (5 stories, 3 minutes each)**
- Requirements: STAR format (Situation, Task, Action, Result)
- Story 1 - Multi-Agent: "Built supervisor-worker system coordinating 5 agents for fraud detection. Result: 94% accuracy, 2.5s latency."
- Story 2 - Human-in-Loop: "Implemented four-eyes approval for banking compliance. Result: 100% regulatory compliance, complete audit trail."
- Story 3 - Evaluation: "Built 50-case golden dataset with fraud expert. Result: Comprehensive testing, 0.04 ECE calibration."
- Story 4 - Integration: "Integrated 5 Phase 2 systems into cohesive platform. Result: End-to-end workflow from PDF to approval."
- Story 5 - Impact: "Reduced manual review time 70% while maintaining 94% accuracy. Result: $500K annual savings (estimated)."
- What to figure out: How to tell compelling stories
- Practice: Rehearse out loud
- Document: Story scripts

**Success Criteria:**
- Demo script polished (10 min)
- Presentation slides professional (15 slides)
- Interview stories ready (5 Ã— 3 min)
- Practiced and confident
- Portfolio presentation complete

---

## DAY 7 (SUNDAY): Phase 2 Completion

**Time:** 2 hours

### SESSION 1: Final Testing (60 min)

**Requirements:**

Comprehensive end-to-end validation:

**Test Category 1: End-to-End Workflows**
- Requirements: Test all user journeys
- Journey 1: Auto-approve path (high confidence â†’ execute)
- Journey 2: Single review path (medium confidence â†’ analyst â†’ execute)
- Journey 3: Four-eyes path (high-risk â†’ 2 analysts â†’ execute)
- Journey 4: Disagreement path (analysts disagree â†’ escalate â†’ manager)
- Journey 5: Feedback path (correction â†’ store â†’ analyze â†’ improve)
- What to figure out: Which journeys most critical
- Document: Test results

**Test Category 2: Error Handling**
- Requirements: Verify graceful degradation
- Error 1: Database down (queue offline, fallback to direct execution with logging)
- Error 2: API timeout (retry logic, eventual failure with alert)
- Error 3: Invalid input (validation catches, clear error message)
- Error 4: Queue full (throttle new items, alert operations)
- What to figure out: Acceptable degradation modes
- Document: Error recovery verification

**Test Category 3: Performance Under Load**
- Requirements: Stress test system
- Load 1: 100 concurrent transactions
- Load 2: 50 pending approvals
- Verify: Response times acceptable (< 5s), No data corruption, Audit logs complete, SLA maintained
- What to figure out: Breaking point, scaling limits
- Document: Performance test results

**Test Category 4: Data Integrity**
- Requirements: Verify data correctness
- Check 1: Audit logs complete (every action logged)
- Check 2: No data loss (queue items persisted)
- Check 3: Timestamps accurate (all times correct)
- Check 4: Referential integrity (foreign keys valid)
- What to figure out: Data quality metrics
- Document: Data integrity verification

**Test Category 5: Security**
- Requirements: Verify security controls
- Test 1: Unauthorized access (401/403 responses)
- Test 2: SQL injection (inputs sanitized)
- Test 3: XSS attacks (outputs escaped)
- Test 4: RBAC enforcement (roles respected)
- What to figure out: Security testing tools
- Document: Security test results

**Bug Fixes:**
- Priority 1 (Critical): Crashes, data loss â†’ Fix immediately
- Priority 2 (High): Incorrect behavior â†’ Fix before completion
- Priority 3 (Medium): Performance issues â†’ Fix if time allows
- Priority 4 (Low): UX improvements â†’ Document for later

**Success Criteria:**
- All test categories passed
- Critical bugs fixed
- Performance acceptable
- Security verified
- Data integrity confirmed

### SESSION 2: Portfolio Finalization (60 min)

**Requirements:**

Complete portfolio materials:

**Deliverable 1: GitHub Repository**
- Requirements: Professional open-source repository
- Files: README.md (overview, features, quick start), ARCHITECTURE.md (system design), API_DOCS.md (API reference), DEPLOYMENT.md (deployment guide), EVALUATION.md (test results, metrics), LICENSE (MIT or Apache 2.0)
- Organization: Clear directory structure, Comprehensive documentation, Examples and demos
- What to figure out: What to include vs exclude
- Document: Repository structure

**Deliverable 2: Portfolio Website Section**
- Requirements: Project showcase page
- Sections: Hero (title, tagline, screenshot), Problem (manual review inefficiency), Solution (AI + human collaboration), Architecture (diagram), Features (multi-agent, vision, structured outputs, evaluation, HITL), Tech Stack (icons + descriptions), Metrics (94% accuracy, 50-case test, 0.04 ECE), Demo (video or screenshots), GitHub (link to repository)
- What to figure out: Visual design, Hosting
- Tools: React, Next.js, Tailwind, Vercel
- Document: Website content

**Deliverable 3: LinkedIn Post**
- Requirements: Project announcement
- Structure: Hook ("Just completed 7-week AI agent development program"), What built (5 production systems), Key features (highlights), Tech stack (skills demonstrated), Impact (fintech-ready, regulatory compliance), Metrics (94% accuracy, 50-case test suite, 4-eyes approval), Call-to-action (GitHub link, open to opportunities)
- Length: 1300 characters max
- What to figure out: Engaging writing style
- Document: LinkedIn post

**Deliverable 4: Resume Updates**
- Requirements: Add Phase 2 projects
- Project 1: "Multi-Agent Fraud Detection System - Developed supervisor-worker architecture coordinating 5 LLM agents. Achieved 94% accuracy with 2.5s latency. Tech: Python, LangChain, OpenAI."
- Project 2: "Human-in-Loop Approval Workflow - Implemented four-eyes review system for banking compliance. Complete audit trail, SLA monitoring. Tech: FastAPI, PostgreSQL, React."
- Project 3: "Agent Evaluation Framework - Built 50-case golden dataset, confidence calibration (ECE 0.04), statistical A/B testing. Tech: Pydantic, LangSmith, Pytest."
- Quantify: 94% accuracy, 50-case test suite, 4-eyes compliance, 70% efficiency gain
- What to figure out: Resume formatting
- Document: Resume bullet points

**Success Criteria:**
- GitHub repository complete
- Portfolio website section live
- LinkedIn post published
- Resume updated
- Portfolio materials professional

---

## ðŸ“š ADDITIONAL RESOURCES

**Human-in-Loop:**
- Book: "Human-in-the-Loop Machine Learning" - https://www.manning.com/books/human-in-the-loop-machine-learning
- Article: "Active Learning" - https://towardsdatascience.com/active-learning-in-machine-learning-525e61be16e5
- Video: "HITL AI Systems" - https://www.youtube.com/watch?v=Yqj7Kyjznh4

**Banking Compliance:**
- Four-Eyes Principle: https://en.wikipedia.org/wiki/Four-eyes_principle
- SOX Compliance: https://www.sarbanes-oxley-101.com/
- Basel III: https://www.bis.org/bcbs/basel3.htm

**Workflow Design:**
- State Machines: https://github.com/pytransitions/transitions
- Queue Systems: https://python-rq.org/
- Audit Logging: https://www.elastic.co/guide/en/ecs/current/ecs-reference.html

**Portfolio:**
- GitHub README: https://github.com/matiassingers/awesome-readme
- Portfolio Sites: https://www.awwwards.com/websites/portfolio/
- Resume Writing: https://www.careercup.com/resume

---

## âœ… WEEK 21 DELIVERABLES

**Documentation:**
- PHASE2_ARCHITECTURE.md - Complete system architecture
- PHASE2_COMPLETION_SUMMARY.md - Phase 2 summary
- DEPLOYMENT_GUIDE.md - Deployment instructions
- API_DOCUMENTATION.md - API reference
- MONITORING_RUNBOOK.md - Operations manual
- WEEK21_SUMMARY.md - Week summary

**Implementation Files (Requirements):**
- approval_queue.py - Queue management (requirements documented)
- four_eyes_workflow.py - Four-eyes approval (requirements documented)
- feedback_collector.py - Feedback integration (requirements documented)
- active_learning.py - Learning loop (requirements documented)
- integration.py - Phase 2 integration (requirements documented)
- Test suite (requirements documented)

**Portfolio Materials:**
- 6 architecture diagrams (PNG + SVG)
- Demo script (10 min)
- Presentation slides (15 slides)
- Interview stories (5 Ã— 3 min)
- GitHub repository (complete)
- Portfolio website section (live)
- LinkedIn post (published)
- Resume updates (Phase 2 projects)

**Understanding:**
- Human-in-loop design patterns
- Four-eyes principle implementation
- Approval workflow architecture
- Feedback loop mechanics
- Active learning methodology
- System integration strategies
- Professional documentation standards

---

## ðŸŽ¯ SUCCESS CRITERIA

**By end of Week 21:**

**Conceptual:**
- Understand when/why to use human-in-loop
- Know four-eyes principle requirements
- Understand approval workflow states
- Know feedback integration patterns
- Understand active learning loop

**Practical:**
- Design approval queue systems
- Implement four-eyes workflows
- Build feedback collection interfaces
- Create active learning pipelines
- Integrate complex multi-component systems
- Write professional documentation
- Prepare portfolio materials

**Portfolio Impact:**
- âœ… Banking compliance demonstrated (FINTECH CRITICAL)
- âœ… Human-AI collaboration expertise
- âœ… Production workflow design
- âœ… System integration capability
- âœ… Professional documentation skills
- âœ… Complete Phase 2 portfolio

---

## ðŸŽ“ PHASE 2 COMPLETION SUMMARY

**7 Weeks Completed (Weeks 15-21):**
- Week 15: Agent Foundations
- Week 16: Advanced Tool Use + Calculator
- Week 17: Multi-Agent Orchestration
- Week 18: Multimodal Vision Agents
- Week 19: Structured Outputs + Java Integration
- Week 20: Comprehensive Evaluation Framework
- Week 21: Human-in-Loop + Professional Documentation

**Total Hours:** 77-84 hours  
**Systems Built:** 5 major production-grade systems  
**Skills Mastered:** Agent design, tool integration, vision AI, type safety, evaluation, HITL, compliance  

**Ready For:**
- Week 22: Fine-tuning + Deployment
- Month 6-7: MLOps + AWS Certification
- Month 8: Capstone (Java + Python)
- Job Applications: Senior AI/ML Engineer

---

**Total Time:** 11-12 hours  
**Fintech Impact:** CRITICAL - Banking requires four-eyes approval  
**Next Week:** Fine-tuning Fundamentals + LoRA