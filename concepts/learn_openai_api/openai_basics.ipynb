{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-18T15:28:39.006501Z",
     "start_time": "2025-12-18T15:28:37.197552Z"
    }
   },
   "source": [
    "# %pip install openai\n",
    "# %pip install --upgrade pip"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (24.3.1)\r\n",
      "Collecting pip\r\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m60.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: pip\r\n",
      "  Attempting uninstall: pip\r\n",
      "    Found existing installation: pip 24.3.1\r\n",
      "    Uninstalling pip-24.3.1:\r\n",
      "      Successfully uninstalled pip-24.3.1\r\n",
      "Successfully installed pip-25.3\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T15:52:32.949377Z",
     "start_time": "2025-12-18T15:52:25.257879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" # Paste your actual key here\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "# ... rest of your codefrom openai import OpenAI\n",
    "\n",
    "respone = client.responses.create(model=\"gpt-5-nano\",input=\"Write a one-sentence bedtime story about a unicorn.\")\n",
    "print(respone.output)"
   ],
   "id": "d29157fa3da7333a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseReasoningItem(id='rs_06db69d93ef5b020006944233a47a881a2a56e0885df355c78', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_06db69d93ef5b02000694423401fc881a29f93a01f3821e506', content=[ResponseOutputText(annotations=[], text='Under a silver moon, a gentle unicorn trotted through a quiet meadow of starlight and lavender, whispering lullabies that tucked every dreamer into a peaceful sleep.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T17:23:48.475836Z",
     "start_time": "2025-12-18T17:23:44.806167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import openai\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "completion = await client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
    "\n",
    "# completion = client.completions.create(model='curie')\n",
    "print(completion.choices[0].text)\n",
    "print(dict(completion).get('usage'))\n",
    "print(completion.model_dump_json(indent=2))\n",
    "\n",
    "# chat_completion = openai.ChatCompletion.create(model=\"gpt-5-nano\",messages=[{\n",
    "#     \"role\" : \"User\",\n",
    "#     \"content\" : \"Write a one-sentence bedtime story about a unicorn.\"}])\n",
    "# print(chat_completion.to_dic())"
   ],
   "id": "d51bf4b68e9f765d",
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRateLimitError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mopenai\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AsyncOpenAI\n\u001B[32m      5\u001B[39m client = AsyncOpenAI()\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m completion = \u001B[38;5;28;01mawait\u001B[39;00m client.chat.completions.create(model=\u001B[33m\"\u001B[39m\u001B[33mgpt-3.5-turbo\u001B[39m\u001B[33m\"\u001B[39m, messages=[{\u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mHello world\u001B[39m\u001B[33m\"\u001B[39m}])\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# completion = client.completions.create(model='curie')\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(completion.choices[\u001B[32m0\u001B[39m].text)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:2678\u001B[39m, in \u001B[36mAsyncCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m   2631\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m   2632\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m   2633\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2675\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = not_given,\n\u001B[32m   2676\u001B[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001B[32m   2677\u001B[39m     validate_response_format(response_format)\n\u001B[32m-> \u001B[39m\u001B[32m2678\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._post(\n\u001B[32m   2679\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m/chat/completions\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   2680\u001B[39m         body=\u001B[38;5;28;01mawait\u001B[39;00m async_maybe_transform(\n\u001B[32m   2681\u001B[39m             {\n\u001B[32m   2682\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m\"\u001B[39m: messages,\n\u001B[32m   2683\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model,\n\u001B[32m   2684\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33maudio\u001B[39m\u001B[33m\"\u001B[39m: audio,\n\u001B[32m   2685\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfrequency_penalty\u001B[39m\u001B[33m\"\u001B[39m: frequency_penalty,\n\u001B[32m   2686\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfunction_call\u001B[39m\u001B[33m\"\u001B[39m: function_call,\n\u001B[32m   2687\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mfunctions\u001B[39m\u001B[33m\"\u001B[39m: functions,\n\u001B[32m   2688\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mlogit_bias\u001B[39m\u001B[33m\"\u001B[39m: logit_bias,\n\u001B[32m   2689\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mlogprobs\u001B[39m\u001B[33m\"\u001B[39m: logprobs,\n\u001B[32m   2690\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_completion_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_completion_tokens,\n\u001B[32m   2691\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmax_tokens\u001B[39m\u001B[33m\"\u001B[39m: max_tokens,\n\u001B[32m   2692\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m: metadata,\n\u001B[32m   2693\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mmodalities\u001B[39m\u001B[33m\"\u001B[39m: modalities,\n\u001B[32m   2694\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mn\u001B[39m\u001B[33m\"\u001B[39m: n,\n\u001B[32m   2695\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mparallel_tool_calls\u001B[39m\u001B[33m\"\u001B[39m: parallel_tool_calls,\n\u001B[32m   2696\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprediction\u001B[39m\u001B[33m\"\u001B[39m: prediction,\n\u001B[32m   2697\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mpresence_penalty\u001B[39m\u001B[33m\"\u001B[39m: presence_penalty,\n\u001B[32m   2698\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprompt_cache_key\u001B[39m\u001B[33m\"\u001B[39m: prompt_cache_key,\n\u001B[32m   2699\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mprompt_cache_retention\u001B[39m\u001B[33m\"\u001B[39m: prompt_cache_retention,\n\u001B[32m   2700\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mreasoning_effort\u001B[39m\u001B[33m\"\u001B[39m: reasoning_effort,\n\u001B[32m   2701\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mresponse_format\u001B[39m\u001B[33m\"\u001B[39m: response_format,\n\u001B[32m   2702\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33msafety_identifier\u001B[39m\u001B[33m\"\u001B[39m: safety_identifier,\n\u001B[32m   2703\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mseed\u001B[39m\u001B[33m\"\u001B[39m: seed,\n\u001B[32m   2704\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mservice_tier\u001B[39m\u001B[33m\"\u001B[39m: service_tier,\n\u001B[32m   2705\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m: stop,\n\u001B[32m   2706\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstore\u001B[39m\u001B[33m\"\u001B[39m: store,\n\u001B[32m   2707\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m: stream,\n\u001B[32m   2708\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mstream_options\u001B[39m\u001B[33m\"\u001B[39m: stream_options,\n\u001B[32m   2709\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtemperature\u001B[39m\u001B[33m\"\u001B[39m: temperature,\n\u001B[32m   2710\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtool_choice\u001B[39m\u001B[33m\"\u001B[39m: tool_choice,\n\u001B[32m   2711\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtools\u001B[39m\u001B[33m\"\u001B[39m: tools,\n\u001B[32m   2712\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_logprobs\u001B[39m\u001B[33m\"\u001B[39m: top_logprobs,\n\u001B[32m   2713\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mtop_p\u001B[39m\u001B[33m\"\u001B[39m: top_p,\n\u001B[32m   2714\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m: user,\n\u001B[32m   2715\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mverbosity\u001B[39m\u001B[33m\"\u001B[39m: verbosity,\n\u001B[32m   2716\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mweb_search_options\u001B[39m\u001B[33m\"\u001B[39m: web_search_options,\n\u001B[32m   2717\u001B[39m             },\n\u001B[32m   2718\u001B[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001B[32m   2719\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m stream\n\u001B[32m   2720\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001B[32m   2721\u001B[39m         ),\n\u001B[32m   2722\u001B[39m         options=make_request_options(\n\u001B[32m   2723\u001B[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001B[32m   2724\u001B[39m         ),\n\u001B[32m   2725\u001B[39m         cast_to=ChatCompletion,\n\u001B[32m   2726\u001B[39m         stream=stream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m   2727\u001B[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001B[32m   2728\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages/openai/_base_client.py:1797\u001B[39m, in \u001B[36mAsyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1784\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1785\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1792\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_AsyncStreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1793\u001B[39m ) -> ResponseT | _AsyncStreamT:\n\u001B[32m   1794\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1795\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=\u001B[38;5;28;01mawait\u001B[39;00m async_to_httpx_files(files), **options\n\u001B[32m   1796\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1797\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages/openai/_base_client.py:1597\u001B[39m, in \u001B[36mAsyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1594\u001B[39m             \u001B[38;5;28;01mawait\u001B[39;00m err.response.aread()\n\u001B[32m   1596\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1597\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1599\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1601\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mRateLimitError\u001B[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
