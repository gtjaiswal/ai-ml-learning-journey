{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-18T19:34:56.210127Z",
     "start_time": "2025-12-18T19:34:54.570370Z"
    }
   },
   "source": [
    "\n",
    "%pip install ollama"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\r\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\r\n",
      "Requirement already satisfied: httpx>=0.27 in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (from ollama) (0.28.1)\r\n",
      "Requirement already satisfied: pydantic>=2.9 in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (from ollama) (2.12.5)\r\n",
      "Requirement already satisfied: anyio in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (4.12.0)\r\n",
      "Requirement already satisfied: certifi in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (2025.11.12)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (1.0.9)\r\n",
      "Requirement already satisfied: idna in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (from httpx>=0.27->ollama) (3.11)\r\n",
      "Requirement already satisfied: h11>=0.16 in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (2.41.5)\r\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (4.15.0)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/garimajaiswal/Learning/AI-ML/ai-ml-learning-journey/.venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (0.4.2)\r\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\r\n",
      "Installing collected packages: ollama\r\n",
      "Successfully installed ollama-0.6.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Method A: Official Python Library**",
   "id": "7b62ccbd69c58061"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T21:26:34.422193Z",
     "start_time": "2025-12-18T21:26:34.374201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from httpx import ConnectError\n",
    "import time\n",
    "import ollama\n",
    "\n",
    "try:\n",
    "    start = time.time()\n",
    "    response = ollama.chat(model=\"llama3.2:3b\",\n",
    "                           messages=[\n",
    "                               {\n",
    "                                   \"role\":\"user\",\n",
    "                                   \"content\":\"explain quantun physics in one sentence\"\n",
    "                               }],\n",
    "\n",
    "                           )\n",
    "    end = time.time()\n",
    "    print(response['message']['content'])\n",
    "    print(f\"Time taken: {end - start:.2f} seconds\")\n",
    "except ConnectionError:\n",
    "    print(\"‚ùå Error: ConnectionError, Could not connect to Ollama.\")\n",
    "    print(\"üëâ Fix: Open your terminal and type 'ollama serve' or open the Ollama app.\")\n",
    "except ConnectError:\n",
    "    print(\"‚ùå Error: Could not connect to Ollama.\")\n",
    "    print(\"üëâ Fix: Open your terminal and type 'ollama serve' or open the Ollama app.\")\n",
    "except ollama.ResponseError as e:\n",
    "    print(f\"‚ùå Error: Model issue. {e.error}\")\n",
    "    print(\"üëâ Fix: Check if you pulled the model using 'ollama pull <model_name>'\")"
   ],
   "id": "654960aee4006257",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error: Could not connect to Ollama.\n",
      "üëâ Fix: Open your terminal and type 'ollama serve' or open the Ollama app.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Method B: Raw Requests (Under the Hood)**",
   "id": "1a318b88bed98821"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:57:56.618443Z",
     "start_time": "2025-12-18T19:57:54.803283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''Since Ollama is just a local web server running on port 11434, you can use standard HTTP requests. This is useful if you don't want to install the ollama package.'''\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "data = {\n",
    "    \"model\": \"llama3.2:3b\",\n",
    "    \"prompt\": \"Explain quantum physics in one sentence.\",\n",
    "    \"stream\": False  # Return the whole answer at once, not piece by piece\n",
    "}\n",
    "\n",
    "response = requests.post(url,json=data)\n",
    "print(response.json()['response'])"
   ],
   "id": "efa46b2093aa33f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum physics is a branch of physics that studies the behavior of matter and energy at an atomic and subatomic level, where the principles of wave-particle duality, superposition, entanglement, and uncertainty govern the interactions between particles and waves.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T21:16:15.472722Z",
     "start_time": "2025-12-18T21:16:02.817175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "models = [\"llama3.2:3b\",\"phi3:mini\",\"mistral:7b\"]\n",
    "message_details=[{\n",
    "            \"role\":\"System\",\n",
    "            \"content\" : \"You are a transaction categorization expert\"\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"User\",\n",
    "            \"content\" : \"Categorize this transaction into category and subcategory with confidence 0-100\"\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"User\",\n",
    "            \"content\" : \"Transaction BOLT $45.00\"\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"User\",\n",
    "                \"format\":\"Category, Subcategory, Confidence\"\n",
    "            }]\n",
    "try:\n",
    "    for model in models:\n",
    "        start = time.time()\n",
    "        response = ollama.chat(\n",
    "            model = model,\n",
    "            messages= message_details\n",
    "        )\n",
    "        end = time.time()\n",
    "        print(f\"model : {model}\\n\")\n",
    "        print(f\"response : {response['message']['content']}\\n\")\n",
    "        print(f\"Time taken: {end - start:.2f} seconds\\n\")\n",
    "except ConnectError:\n",
    "    print(\"‚ùå Error: Could not connect to Ollama.\")\n",
    "    print(\"üëâ Fix: Open your terminal and type 'ollama serve' or open the Ollama app.\")\n",
    "except ollama.ResponseError as e:\n",
    "    print(f\"‚ùå Error: Model issue. {e.error}\")\n",
    "    print(\"üëâ Fix: Check if you pulled the model using 'ollama pull <model_name>'\")"
   ],
   "id": "31ac13d24a8b6bf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : llama3.2:3b\n",
      "\n",
      "response : I would categorize the transaction as:\n",
      "\n",
      "Category: Personal Spending\n",
      "Subcategory: Entertainment/Recreation\n",
      "\n",
      "Confidence Level: 90%\n",
      "\n",
      "The reason for this categorization is that the term \"BOLT\" is often associated with a fast-food restaurant chain, and given the amount of $45.00, it's likely that the transaction is related to purchasing food or drinks from the restaurant. The high confidence level is due to the strong likelihood that this is a purchase at a fast-food establishment.\n",
      "\n",
      "Time taken: 3.20 seconds\n",
      "\n",
      "model : phi3:mini\n",
      "\n",
      "response : Category: Electrical Home Improvement Services  \n",
      "\n",
      "Subcategory: Lighting Fixt extranjeros para mejorar la luz interior y exterior del hogar (foreign light fixture installation for improving home's interior and exterior light) - Confidence: 95  \n",
      "\n",
      "Note: While the confidence level is high, without additional context about what \"Bolt\" refers to in this scenario or industry-specific codes that may exist within certain regions, it might be possible there could be some ambiguity.\n",
      "\n",
      "Time taken: 3.68 seconds\n",
      "\n",
      "model : mistral:7b\n",
      "\n",
      "response :  Category: Shopping and Retail (95%)\n",
      "Subcategory: Clothing, Shoes & Accessories (80%)\n",
      "\n",
      "The transaction made to BOLT for the amount of $45.00 is likely a purchase related to clothing, shoes or accessories, as these are common items sold by retailers like BOLT. However, without additional context such as specific product details, it's not possible to provide an exact subcategory with 100% confidence.\n",
      "\n",
      "Time taken: 5.76 seconds\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T21:22:25.966633Z",
     "start_time": "2025-12-18T21:22:20.387984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "models = [\"llama3.2:3b\",\"abc_model\"]\n",
    "message_details=[{\n",
    "            \"role\":\"System\",\n",
    "            \"content\" : \"You are a transaction categorization expert\"\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"User\",\n",
    "            \"content\" : \"Categorize this transaction into category and subcategory with confidence 0-100\"\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"User\",\n",
    "            \"content\" : \"Transaction BOLT $45.00\"\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"User\",\n",
    "                \"format\":\"Category, Subcategory, Confidence\"\n",
    "            }]\n",
    "try:\n",
    "    for model in models:\n",
    "        start = time.time()\n",
    "        response = ollama.chat(\n",
    "            model = model,\n",
    "            messages= message_details\n",
    "        )\n",
    "        end = time.time()\n",
    "        print(f\"model : {model}\\n\")\n",
    "        print(f\"response : {response['message']['content']}\\n\")\n",
    "        print(f\"Time taken: {end - start:.2f} seconds\\n\")\n",
    "except ConnectError:\n",
    "    print(\"‚ùå Error: Could not connect to Ollama.\")\n",
    "    print(\"üëâ Fix: Open your terminal and type 'ollama serve' or open the Ollama app.\")\n",
    "except ollama.ResponseError as e:\n",
    "    print(f\"‚ùå Error: Model issue. {e.error}\")\n",
    "    print(\"üëâ Fix: Check if you pulled the model using 'ollama pull <model_name>'\")"
   ],
   "id": "5b1ee3aa2863c554",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model : llama3.2:3b\n",
      "\n",
      "response : Based on the provided information, I would categorize this transaction as:\n",
      "\n",
      "**Category:** Shopping\n",
      "**Subcategory:** Groceries/Personal Care\n",
      "\n",
      "I assign a confidence level of **90%**, as the transaction involves a purchase from an online retailer (BOLT) with no clear indication of a service-based transaction or any other category. The lack of specific details makes it challenging to categorize the transaction more precisely, but based on common e-commerce platforms and products sold by retailers like BOLT, I lean towards groceries/personal care as a reasonable assumption.\n",
      "\n",
      "Time taken: 5.53 seconds\n",
      "\n",
      "‚ùå Error: Model issue. model 'abc_model' not found\n",
      "üëâ Fix: Check if you pulled the model using 'ollama pull <model_name>'\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
